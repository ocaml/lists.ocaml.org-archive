<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [ocaml-platform] Benchmarking in OPAM
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:platform%40lists.ocaml.org?Subject=Re%3A%20%5Bocaml-platform%5D%20Benchmarking%20in%20OPAM&In-Reply-To=%3C20130319.180741.183612882661784889.Christophe.Troestler%40umons.ac.be%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000303.html">
   <LINK REL="Next"  HREF="000356.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[ocaml-platform] Benchmarking in OPAM</H1>
    <B>Christophe TROESTLER</B> 
    <A HREF="mailto:platform%40lists.ocaml.org?Subject=Re%3A%20%5Bocaml-platform%5D%20Benchmarking%20in%20OPAM&In-Reply-To=%3C20130319.180741.183612882661784889.Christophe.Troestler%40umons.ac.be%3E"
       TITLE="[ocaml-platform] Benchmarking in OPAM">Christophe.Troestler at umons.ac.be
       </A><BR>
    <I>Tue Mar 19 17:07:41 GMT 2013</I>
    <P><UL>
        <LI>Previous message: <A HREF="000303.html">[ocaml-platform] Benchmarking in OPAM
</A></li>
        <LI>Next message: <A HREF="000356.html">[ocaml-platform] Benchmarking in OPAM
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#357">[ date ]</a>
              <a href="thread.html#357">[ thread ]</a>
              <a href="subject.html#357">[ subject ]</a>
              <a href="author.html#357">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>On Thu, 14 Mar 2013 10:48:57 +0000, Anil Madhavapeddy wrote:
&gt;<i> 
</I>&gt;<i> &gt; Finally, we realized that we really need two distinct kinds of
</I>&gt;<i> &gt; benchmarking software:
</I>&gt;<i> &gt; - one &quot;benchmark library&quot; that is solely meant to run performance
</I>&gt;<i> &gt; tests and return the results (will be used by and linked with the
</I>&gt;<i> &gt; benchmark programs, so recompiled at each compiler change, so should
</I>&gt;<i> &gt; be rather light if possible)
</I>&gt;<i> &gt; - one &quot;benchmark manager&quot; that compares results between different
</I>&gt;<i> &gt; runs, plots nice graphics, stores results over time or send them to a
</I>&gt;<i> &gt; CI server, format them in XML or what not. This one is run from the
</I>&gt;<i> &gt; system compiler and can have arbitrarily large feature sets and
</I>&gt;<i> &gt; dependencies.
</I>&gt;<i> &gt; 
</I>&gt;<i> &gt; I believe a similar split would be meaningful for unit testing as
</I>&gt;<i> &gt; well. Of course, if you're considering daily automated large-scale
</I>&gt;<i> &gt; package building and checking, instead of tight feedback loops, it is
</I>&gt;<i> &gt; much less compelling to force a split, you can just bundle the two
</I>&gt;<i> &gt; kind of features under the same package.
</I>&gt;<i> 
</I>&gt;<i> The split you describe is generally good discipline, as it encourages
</I>&gt;<i> library authors to encode more small benchmarks that can be called from
</I>&gt;<i> larger tools.
</I>&gt;<i> 
</I>&gt;<i> The benchmark manager is definitely something we want to have in the
</I>&gt;<i> OPAM hosted test system.  It's very difficult to get representative
</I>&gt;<i> benchmark results without a good mix of architectures and operating
</I>&gt;<i> systems, and we're going to pepper lots of odd setups into OPAM (and
</I>&gt;<i> eventually have the facility to crowdsource other people's machines
</I>&gt;<i> into the build pool, to make it easier to contribute resources).
</I>&gt;<i> 
</I>&gt;<i> So for the moment, focussing on the benchmark library would seem to
</I>&gt;<i> be the best thing to do: I've not really used any of them, and would
</I>&gt;<i> be interested in knowing what I ought to adopt for (e.g.) the Mirage
</I>&gt;<i> protocol libraries.  Once we have that in place, the OPAM test integration
</I>&gt;<i> should be much more straightforward.
</I>
Maybe this is also a good time to promote a single benchmarking
framework.  As the other two libraries mentioned by T&#246;r&#246;k Edwin,
Benchmark computes the mean and std deviation &#8212; it just does not
expose them to the user.  All these libs have a lot in common and,
IMHO, it would be best to merge the features of the 3 libraries.  I
agree with the proposed split.  As I understand it:

- Benchmark: type defining what is a benchmarking sample, functions to
  write and read it to a simple format [more complex formats,
  e.g. XML, can be supported by Benchmark_manager], functions to
  perform tests (min number of samples, min running time).

- Benchmark_manager: all the rest. Statistical tests,... as already
  described by Gabriel.

If you need the name, I agree to deprecate the Benchmark module once a
replacement following those lines has seen the light.  Also, if I can
be of some help, just let me know (just do not want to take the lead
for lack of time).

Best,
C.
</PRE>





















<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000303.html">[ocaml-platform] Benchmarking in OPAM
</A></li>
	<LI>Next message: <A HREF="000356.html">[ocaml-platform] Benchmarking in OPAM
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#357">[ date ]</a>
              <a href="thread.html#357">[ thread ]</a>
              <a href="subject.html#357">[ subject ]</a>
              <a href="author.html#357">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://lists.ocaml.org/listinfo/platform">More information about the Platform
mailing list</a><br>
</body></html>
